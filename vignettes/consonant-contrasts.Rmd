---
title: "Consonant Place-of-Articulation Contrasts"
author: "Patrick F. Reidy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Consonant Place-of-Articulation Contrasts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = FALSE}
library(magrittr)
```


This vignette demonstrates how to apply functions from the `phoneigen` package
to compute low-dimensional representations from high-dimensional spectral 
representations of voiceless consonants. Specifically, these functions will be 
applied to American English-speaking adults' productions of two pairs of 
consonants that differ in terms of their place of articulation: /s/-vs.-/ʃ/
and /t/-vs.-/k/.



## Data

The speech production data used for this vignette are provided by the 
[Learning to Talk Project](http://learningtotalk.org) (NIDCD grant R01-02932;
Principal Investigators Jan Edwards, Mary E. Beckman, and Benjamin Munson),
a longitudinal study of phonological and lexical development in preschool
children. As a part of this project, adult speakers were also tested in order
to assess the adult production norms in the ambient community in which the
child participants were being raised. Only productions from adult speakers
are used in this vignette.

```{r, echo = FALSE, results = "hide"}
n_participants <-
  phoneigen::SibilantFricatives() %>%
  dplyr::pull(Participant) %>%
  dplyr::n_distinct()
n_female <-
  phoneigen::SibilantFricatives() %>%
  dplyr::distinct(Session) %>%
  dplyr::pull(Session) %>%
  stringr::str_subset(pattern = "F.*2$") %>%
  dplyr::n_distinct()
n_male <-
  phoneigen::SibilantFricatives() %>%
  dplyr::distinct(Session) %>%
  dplyr::pull(Session) %>%
  stringr::str_subset(pattern = "M.*2$") %>%
  dplyr::n_distinct()
mean_age <-
  phoneigen::SibilantFricatives() %>%
  dplyr::distinct(Session) %>%
  dplyr::pull(Session) %>%
  stringr::str_subset(pattern = "2$") %>%
  substring(first = 5, last = 6) %>%
  as.numeric() %>%
  mean()
```

`r n_participants` adult native speakers of American English (`r n_female`
female, `r n_male` male; mean age `r mean_age` years) completed a 
picture-prompted word repetition task. The test words for this task were 
selected in order to elicit multiple productions of each of the target 
consonants /s, ʃ, t, k/ in word-initial position, across a variety of following
vowel contexts. On each trial, a picture of the referent of the test word
was displayed on a computer screen, and then an audio recording of the test
word spoken by a female adult native speaker was played over loudspeakers.
The participants were asked to repeat, into a microphone, the word that they
heard. Each experiment session was recorded digitally at 44.1 kHz sampling rate
and 16 bit resolution. Each participant completed two sessions.
A phonetically-trained research assistant manually annotated the onset of
frication (for each sibilant fricative production), the burst (for each stop
production), and the onset of voicing (for each production).

The data for the sibilant fricative /s, ʃ/ and the stop /t, k/ productions are
accessed by calling `phoneigen::SibilantFricatives()` and `phoneigen::StopBursts()`,
respectively.

```{r}
phoneigen::SibilantFricatives()
```

```{r}
phoneigen::StopBursts()
```

Each row in these data sets corresponds to a production that will be included
in the analyses below. Both data sets are similar in structure, with 
`r ncol(phoneigen::SibilantFricatives())` variables:

1. `File`: the name of the WAV file found in `data-raw/wav-sibilants` or
    `data-raw/wav-stops` that contains the acoustic data for the production
    that was extracted from the original recording of the session (from 20 ms
    prior to `TaggedOnset` or `TaggedBurst` until 60 ms following `TaggedVOT`)
1. `Participant`: a 4-character alphanumeric identifier for the participant
1. `Session`: a 9-character alphanumeric identifier for the session
1. `SessionCompleted`: the date and time that the session was completed
1. `TaggingCompleted`: the date and time that the research assistant finished
    annotating the acoustic landmarks for each production in the recording of
    the session
1. `Trial`: the trial number of the test word within the session
1. `Orthograph`: the orthgraphic transcription of the test word
1. `TargetC`: the WorldBet transcription of the target consonant
1. `TargetV`: the WorldBet transcription of the vowel following the target
    consonant
1. `TaggedOnset` or `TaggedBurst`: the time of the onset of frication or of the
    burst in the original recording of the full session
1. `TaggedVOT`: the time of the onset of voicing in the original recording of 
    the full session
1. `Onset` or `Burst`: the time of the onset of frication or of the burst in
    the recording `File`
1. `VOT`: the time of the onset of voicing in the recording `File`
1. `ExcitationPattern`: the values of the excitation pattern computed from the
    production of the `TargetC`

An excitation pattern is a psychoacoustic spectral representation that may be 
computed by passing a Fourier spectrum through a filter bank that models the 
auditory periphery. For the sibilant fricative productions, an excitation 
pattern was computed from the middle half of the interval from `Onset` to
`VOT`; for the stop productions, an excitation pattern was computed from the
interval from 5 ms prior to `Burst` to 20 ms after `Burst`. For both types of
production, the interval of interest was first extracted with a rectangular
window, and an eighth-order multitaper spectrum was computed (time-bandwidth
parameter nW = 4). This spectrum was then passed through a filter bank that
comprised 361 fourth-order gammatone filters. The center frequencies of these
filters were evenly spaced from 3 to 39 (i.e., 0.1 interchannel separation)
along the equivalent rectangular bandwidth (ERB) scale---a logarithmic
transformation of the physical hertz scale that models the tonotopic mapping
of the basilar membrane. The bandwidths of the filters increased in proportion 
to their center frequencies; hence, filters centered at higher frequencies were 
wider than those centered at lower frequencies, which models the differential
frequency selectivity of the auditory periphery. Each channel in this filter
bank can thus be thought of as the frequency tuning properties of a narrow
cross-section of the basilar membrane, and the filter bank, as a whole, may be
thought of as a sequence of such cross-sections spaced evenly along the length
of the basilar membrane. An excitation pattern results from applying this
filter bank to an input spectrum, summing the energy within the signal output by
each channel, and associating those energy values to the center frequencies of 
the channels in the filter bank. Hence, an excitation pattern is a function from
frequencies on the ERB scale to energy values---a pattern of excitation across
a sequence of "auditory filters". 

In the following two sections, the excitation patterns are input to the
Laplacian Eigenmaps algorithm in order to learn low-dimensional representations
that characterize the /s/-vs.-/ʃ/ and the /t/-vs.-/k/ contrast, respectively.





## Sibilant fricatives

The sibilant fricatives /s, ʃ/ are articulated by raising the tongue toward the
roof of the mouth, so as to form a narrow constriction in the oral cavity. 
Turbulence noise sources are generated when the air flowing through this
linguopalatal constriction becomes turbulent and when this turbulent jet 
impinges on the teeth, downstream from the constriction. These noise sources
excite the cavity anterior to the constriction; however, the cavity posterior
to the constriction is not excited because it is acoustically decoupled, due
to the narrow aperture of the constriction. Consequently, sibilant frication
is characterized spectrally by concentrations of energy at high frequencies,
and the difference in place of articulation that differentiates /s/ and /ʃ/
is represented primarily in terms of the range of frequencies in which energy 
is concentrated: compared with /ʃ/, the more anterior place of articulation 
for /s/ entails a smaller front-cavity volume and thus resonances at higher 
frequencies. This spectral difference between /s/ and /ʃ/ is  illustrated in 
Figure 1 below, which shows excitation patterns computed from participant 
A50N's productions of the initial sibilants in the words 'sister' (black line) 
and 'shoes' (grey line).

```{r, echo = FALSE, results = "hide"}
fig1_caption <- stringr::str_c(
  "Figure 1: Excitation patterns computed from A50N's productions of",
  "/s/ (black) and /ʃ/ (grey).",
  sep = " "
)
```

```{r, echo = FALSE, fig.cap = fig1_caption, fig.width = 7, fig.height = 3.5, fig.retina = 2, fig.align = "center"}
tibble::tibble(ERB = seq(from = 3, to = 39, by = 0.1),
               s = phoneigen::SibilantFricatives()[[15, "ExcitationPattern"]],
               sh = phoneigen::SibilantFricatives()[[20, "ExcitationPattern"]]) %>%
  dplyr::mutate(s = 10 * log10(s / min(s)),
                sh = 10 * log10(sh / min(sh))) %>%
  ggplot2::ggplot(mapping = ggplot2::aes(x = ERB)) +
  ggplot2::theme_bw() +
  ggplot2::xlab("Frequency (ERB scale)") +
  ggplot2::ylab("Excitation (dB)") +
  ggplot2::geom_line(mapping = ggplot2::aes(y = sh), color = "gray75") +
  ggplot2::geom_line(mapping = ggplot2::aes(y = s), color = "black")
```

### Spectral moments

Spectral representations, such as the excitation patterns shown above, pose a
problem to subsequent phonetic analyses because of their high dimensionality.
A commonly adopted solution to this problem in the phonetics literature has
been to transform a spectral representation through a small number of 
pre-determined functions that yield measures of the shape of the spectrum.
One set of functions that has often been used to characterize sibilant
fricatives (and voiceless obstruents, more generally) are _spectral moments_,
which are based on the well-grounded mathematical theory of moments of a 
probability mass function. Because a power spectrum is a non-negative function, 
it may be normalized by its sum so that it sums to 1. If `p` denotes a vector
of probabilities assigned over `x`, then there are three classes of moments that
are useful for characterizing the shape of `p`:

1. Moments: the `n`th _moment_ is defined as `sum(p * x^n)`
2. Central moments: the `n`th _central moment_ is defined as 
    `sum(p * (x - mu)^n)`, where `mu` is the first moment of `p`
3. Standardized moments: the `n`th _standardized moment_ is defined as
    `sum(p * ((x - mu) / sigma)^n)`, where `mu` is as above and `sigma^2` is
    the second central moment of `p`

Spectral moments are a collection of all three types of moments defined above.
The quantities most commonly used as spectral moments are the first moment
(mean or centroid), which indicates the center of gravity of the distribution 
along the frequency scale; the second central moment (the variance) or its 
square root (the standard deviation), both of which indicate the spread of the 
distribution; the third standardized moment (skewness), a unitless quantity
that indicates the asymmetry of the distribution; and the fourth standardized
moment (kurtosis), a unitless quantity that indicates the heaviness of the
tails of the distribution.

Although spectral moments, as representations for sibilant fricatives, have
been critiqued for being ambiguously interpretable in terms of the underlying
articulation (e.g., Koenig, Shadle, Preston & Mooshammer, 2013) and for not
capturing subtle spectral characteristics that may be perceptually salient
(e.g., Jannedy & Weirich, 2017), we nonetheless take them as the jumping-off
point for our discussion of low-dimensional representations of sibilant 
fricatives because of their ease of computation, their canonical mathematical 
interpretation, and their sheer pervasiveness in the phonetics literature.
For example, spectral moments have been used to represent /s/ and /ʃ/ in
studies that investigated place-of-articulation differences (e.g., Forrest,
Weismer, Milenkovic & Dougall, 1988; Jongman, Wayland & Wong, 2000),
developmental differences (e.g., Li, 2012; Nissen & Fox, 2005; Nittrouer,
Studdert-Kennedy & McGowan, 1989), sex-related differences (e.g., Fox & Nissen,
2005; Romeo, Hazan & Pettinato, 2013), among others.

The literature suggests that centroid and skewness are the two most reliable 
spectral moments for differentiating the place of articulation of /s/ vs. /ʃ/
(e.g., Forrest et al., 1988; Jongman et al., 2000; Shadle & Mair, 1996). This
consensus is perhaps unsurprising, given that centroid indexes the location of
energy concentration along the frequency scale, which is expected to vary
between sibilant fricatives due to differences in front cavity volume that are
concomittant with differences in place of articulation; and that skewness tends
to be highly correlated with centroid (Blacklock, 2004), which is likely due
to the finite support of spectral reprenstations, imposed by the sampling rate
of the acoustic waveform.

The left panel of Figure 2 below plots the sibilant fricatives' skewness values
against their centroid values. Here, it is apparent differences in place of
articulation and in talker sex are indicated by the first and third spectral
moments; however, these measures do not separate the productions in terms of
either place of articulation or talker sex. Furthermore, both spectral moments
simultaneously indicate two distinct dimensions of information conveyed in the 
acoustics of sibilant fricatives: the linguistic dimension of the target 
consonant, and the indexical dimension of the talker's sex. For example, along
the centroid dimension, the cluster centers are ordered such that males' /ʃ/ <
females' /ʃ/ < males' /s/ < females' /s/. The separation between the 
place-of-articulation categories may be enhanced through normalizing the 
centroid and skewness values by subtracting each talkers' mean centroid and 
mean skewness values, respectively, as is shown in the right panel of Figure 2;
however, an unappealing effect of this normalization is that indexical 
differences that indicate talker sex are washed away.

```{r, echo = FALSE, results = "hide"}
fig2_caption <- stringr::str_c(
  "Figure 2: Distributions of skewness and centroid (left panel: raw values,",
  "right panel: normalized values) computed from the sibilant fricatives'",
  "excitation patterns. Ellipses indicate 95% confidence regions,",
  "assuming a bivariate t-distribution.",
  sep = " "
) 
```

```{r, echo = FALSE, fig.cap = fig2_caption, fig.width = 7, fig.height = 5, fig.retina = 2, fig.align = "center"}
Centroid <- function(p, x = seq(from = 3, to = 39, by = 0.1)) {
  p <- p / sum(p)
  sum(p * x)
}

StdDeviation <- function(p, x = seq(from = 3, to = 39, by = 0.1)) {
  p <- p / sum(p)
  sqrt(sum(p * (x - Centroid(p, x))^2))
}

Skewness <- function(p, x = seq(from = 3, to = 39, by = 0.1)) {
  p <- p / sum(p)
  sum(p * ((x - Centroid(p, x)) / StdDeviation(p, x))^3)
}

Kurtosis <- function(p, x = seq(from = 3, to = 39, by = 0.1)) {
  p <- p / sum(p)
  sum(p * ((x - Centroid(p, x)) / StdDeviation(p, x))^4)
}

Moments <-
  phoneigen::SibilantFricatives() %>%
  dplyr::mutate(
    Centroid = purrr::map_dbl(ExcitationPattern, Centroid),
    StdDeviation = purrr::map_dbl(ExcitationPattern, StdDeviation),
    Skewness = purrr::map_dbl(ExcitationPattern, Skewness),
    Kurtosis = purrr::map_dbl(ExcitationPattern, Kurtosis),
    Sex = substring(Session, first = 7, last = 7)
  ) %>%
  dplyr::group_by(Participant) %>%
  dplyr::mutate(
    CentroidMean = mean(Centroid),
    StdDeviationMean = mean(StdDeviation),
    SkewnessMean = mean(Skewness),
    KurtosisMean = mean(Kurtosis)
  ) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(
    CentroidNormed = Centroid - CentroidMean,
    StdDeviationNormed = StdDeviation - StdDeviationMean,
    SkewnessNormed = Skewness - SkewnessMean,
    KurtosisNormed = Kurtosis - KurtosisMean
  )

M1M3_raw <- 
  Moments %>%
  ggplot2::ggplot(mapping = ggplot2::aes(x = Centroid, y = Skewness,
                                         shape = TargetC, linetype = TargetC,
                                         color = Sex)) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom", legend.direction = "horizontal") +
  ggplot2::xlab("Centroid (ERB)") +
  ggplot2::ylab("Skewness") +
  ggplot2::scale_x_continuous(position = "top") +
  ggplot2::scale_shape_manual(name = "", 
                              values = c("s" = 1, "S" = 5), 
                              labels = c("/ʃ/", "/s/")) +
  ggplot2::scale_color_manual(name = "", 
                              values = c("F" = "#E98300", "M" = "#0039A6"),
                              labels = c("Female", "Male")) +
  ggplot2::scale_linetype_manual(name = "", 
                                 values = c("s" = "dashed", "S" = "dotted"),
                                 labels = c("/ʃ/", "/s/")) +
  ggplot2::geom_point(size = 3) +
  ggplot2::stat_ellipse(type = "t", level = 0.95)

M1M3_normed <-
  Moments %>%
  ggplot2::ggplot(mapping = ggplot2::aes(x = CentroidNormed, 
                                         y = SkewnessNormed,
                                         shape = TargetC, linetype = TargetC,
                                         color = Sex)) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom", legend.direction = "horizontal") +
  ggplot2::xlab("Normalized centroid (ERB)") +
  ggplot2::ylab("Normalized skewness") +
  ggplot2::scale_x_continuous(position = "top") +
  ggplot2::scale_y_continuous(position = "right") +
  ggplot2::scale_shape_manual(name = "", 
                              values = c("s" = 1, "S" = 5), 
                              labels = c("/ʃ/", "/s/")) +
  ggplot2::scale_color_manual(name = "", 
                              values = c("F" = "#E98300", "M" = "#0039A6"),
                              labels = c("Female", "Male")) +
  ggplot2::scale_linetype_manual(name = "", 
                                 values = c("s" = "dashed", "S" = "dotted"),
                                 labels = c("/ʃ/", "/s/")) +
  ggplot2::geom_point(size = 3) +
  ggplot2::stat_ellipse(type = "t", level = 0.95)

grid::grid.newpage()
grid::pushViewport(grid::viewport(layout = grid::grid.layout(1, 2)))
print(M1M3_raw, vp = grid::viewport(layout.pos.col = 1))
print(M1M3_normed, vp = grid::viewport(layout.pos.col = 2))
```



### Laplacian eigenmaps

Whereas the method of spectral moments projects high-dimensional data into a 
low-dimensional space whose dimensions reflect pre-determined shape features 
of each high-dimensional observation, the Laplacian eigenmaps algorithm 
seeks instead to uncover the structure of a low-dimensional object near which
the data lie in the high-dimensional space. The manuscript associated with
the `phoneigen` package (Plummer & Reidy, submitted) thoroughly develops the
mathematical concepts necessary to understand the Laplacian eigenmaps
algorithm; so, this vignette focuses on the computational steps necessary to
apply the Laplacian eigenmaps algorithm to a data set that is structured
similarly to `phoneigen::SibilantFricatives()`, i.e. such that the 
high-dimensional data are found in a single (list-)column and the other columns
denote metadata for the high-dimensional data that uniquely identify each
high-dimensional observation.

The `phoneigen` package provides a number of functions that may be chained
together using the forward-pipe operator `%>%` from the `magrittr` package.
The functions `phoneigen::CartesianSquare`, `phoneigen::WeightEdgesIf`, and
`phoneigen::AdjacencyMatrix` facilitate the construction of a symmetric
weighted adjacency matrix from a `data.frame`-like representation of the
raw data. Furthermore, these functions perform all of the bookkeeping necessary
when reshaping data from a `data.frame`-like representation to a `matrix`
representation.

The first step is to pass the data set to `phoneigen::CartesianSquare`, which
additionally takes a comma-separated list of unquoted variable names as 
arguments. `phoneigen::CartesianSquare` selects just the columns matched by the
unquoted variable names and then returns the cross product of that restricted
data set with itself. The motivation for `phoneigen::CartesianSquare` is to
construct a vectorized matrix representation of the data, which will ultimately
be reshaped into a square matrix. Hence, the unquoted variable names passed
to `phoneigen::CartesianSquare` should include the variables that uniquely
identify each high-dimensional observation and the variables that will be
used to compute the edge-weights of the graph. For the `phoneigen::SibilantFricatives()`
data set, the relevant variables are thus `Participant`, `Session`, `Trial`,
`Orthography`, and `ExcitationPattern`. The data set returned by
`phoneigen::CartesianSquare` has twice as many columns as the input data set,
with their names determined by suffixing the input unquoted variables with
`_i` and `_j`, respectively. The motivating idea for `phoneigen::CartesianSquare`
is that the variables suffixed by `_i` and `_j` determine the row and column
positions, respectively, of a cell in a square matrix.

```{r, collapse = TRUE}
squared <-
  phoneigen::CartesianSquare(
    x = phoneigen::SibilantFricatives(),
    Participant, Session, Trial, Orthography, ExcitationPattern
  )

dplyr::select(squared, Session_i, Orthography_i, Session_j, Orthography_j)
```

Once the frame of a vectorized square matrix has been created with
`phoneigen::CartesianSquare`, the weights of edges within that matrix may be
added with `phoneigen::WeightEdgesIf`, which takes as arguments a data frame,
`x`; an optional unquoted variable name, `weights`, for the weight 
values (if `weights` is missing at call time, then the weight values are 
assigned to a column named `W_ij`); an unquoted expression,
`condition`, which, within `x`, evaluates to a logical vector; and
an unquoted expression, `values`, which, within `x`, evaluates to
a numeric vector.

While the examples in the mathematical development of Laplacian eigenmaps
constructed the graph edges through a nearest neighbors search based on 
Euclidean distance, this example will instead construct graph edges based on
the identity of the talker who produced the target consonant and the identity
of the lexical item within which the target consonant was produced. Furthermore,
the edge weights will be derived, not from Euclidean distance between two
adjacent excitation patterns, but by an information-theoretic distance that 
treats the excitation patterns as discrete probability mass functions.
Specifically, the Jeffrey divergence---a symmetric version of the Kullback-Leibler
divergence---will be used. We first define this divergence measure as a 
generic function that may be applied either to two numeric vectors or to
two lists of numeric vectors.

```{r}
Jeffrey <- function(i, j) {
  UseMethod("Jeffrey", i)
}

Jeffrey.numeric <- function(i, j) {
  i <- i/sum(i)
  j <- j/sum(j)
  sum((i-j) * log(i/j))
}

Jeffrey.list <- function(i, j) {
  purrr::map2_dbl(i, j, Jeffrey.numeric)
}
```

With the `Jeffrey` family of functions defined, it is now possible to
write a compact block of code that assigns weights to edges between nodes. The
graph structure that will be used has edges between all productions produced
by a given talker (i.e., the within-talker subgraph is complete). Productions
are aligned across talkers if they are productions of the same word; i.e., the
within-talker manifolds are aligned so that productions of the same target
word are in correspondence. If two productions `i` and `j` are 
connected, then the weight of their edge is equal to `exp(-Jeffrey(i, j))`,
a measure of similarity that is valued within the interval `[0, 1]`. If two
productions do not pass the tests indicated by `condition` then their edge
weight is `0`. 

After initialization with `phoneigen::WeightEdgesIf`, the graph's edge weights
are normalized, such that if `W(i,j)` is the initial weight between nodes `i`
and `j`, then the normalized weight is 
`(1/n) * W(i,j) / sqrt(E_x[W(i,x)] * E_x[W(j,x)])`, where `n` is the number of
nodes in the graph and `E_x` is the expected value taken over nodes in the graph.
This normalization is performed by the function `phoneigen::Normalize`. We
mention that we have run the Laplacian eigenmaps analyses without such 
normalization, and the qualitative pattern of the results are not affected by 
excluding it. We include it here because it is necessary for certain applications
of Laplacian eigenmaps (e.g., its out-of-vocabulary extension presented by
Bengio, Paiment, Vincent, Delalleau, Le Roux & Ouimet, 2004), which are beyond
the scope of this introductory tutorial.

```{r, collapse = TRUE}
weighted <-
  phoneigen::WeightEdgesIf(
    x = squared,
    condition = Participant_i == Participant_j | Orthography_i == Orthography_j,
    values = exp(-Jeffrey(ExcitationPattern_i, ExcitationPattern_j))
  ) %>%
  phoneigen::Normalize()

dplyr::select(weighted, Session_i, Orthography_i, Session_j, Orthography_j, W_ij)

dplyr::filter(weighted, Participant_i != Participant_j & Orthography_i != Orthography_j) %>%
  dplyr::select(Session_i, Orthography_i, Session_j, Orthography_j, W_ij)
```

Once the (normalized) weights are assigned to edges, the values of the weights variable,
`W_ij`, must be reshaped into a square matrix. This task is performed by
the function `phoneigen::AdjacencyMatrix`, which takes as arguments a data frame,
`x`; an optional list of unquoted variables that when suffixed with `_i` match
variable names in `x` (if this list of unquoted variables is missing at call time,
then the variable names are inferred from the names of `x`); and an optional 
unquoted variable, `weights`, which should match the variable in `x` whose 
values denote the edge weights of the graph (if `weights` is missing at call time, 
then the name of the weights variable is assumed to be `W_ij`). The row and column 
names of the resulting matrix are determined by pasting together the values of 
the unquoted variables provided at call time. Furthermore, to ensure that the
resulting matrix is symmetrix, the `phoneigen::AdjacencyMatrix` arranges the 
rows of `x` according to the order of unquoted variables provided at call time.

```{r, collapse = TRUE}
adjacency_matrix <-
  phoneigen::AdjacencyMatrix(
    x = weighted, 
    Participant, Session, Trial, Orthography
  )

# Equivalent to:
# adjacency_matrix <-
#   phoneigen::AdjacencyMatrix(x = weighted)

isSymmetric(adjacency_matrix)

adjacency_matrix[1:5, 1:2]
```

The Laplacian eigenvectors of the graph's `adjacency_matrix` are computed by
the function `phoneigen::LaplacianEigenmaps`. Internally, this function is
powered by the functions `phoneigen::L` and `phoneigen::D`, which compute the
graph's Laplacian and degree matrices, respectively, which are then passed to
`geigen::geigen` to solve the generalized eigenvalue problem. 
`phoneigen::LaplacianEigenmaps` returns a tibble that has as many rows as the
original data set (in this case, `phoneigen::SibilantFricatives()`). The 
`Eigenvector` variable assigns a name to each Laplacian eigenvector, beginning
with `e0`. The `Eigenvalue` variable denotes the eigenvalues associated with
the Laplacian eigenvectors; the arrows of the returned tibble are arranged in
increasing order of `Eigenvalue`s. Finally, the `Projection` variable is a 
list-column of tibbles that have as many rows as the original data set and
two variables that denote the projection of the data onto a Laplacian eigenvector:
a variable named `X` whose values are taken from the row names of the 
`adjacency_matrix`, which should uniquely identify observations in the original
data; and a variable with the same name as the corresponding `Eigenvector`, that
denotes the values of that Laplacian eigenvector, i.e. a one-dimensional
embedding of the data denoted by `X`.

```{r, collapse = TRUE}
eigenmaps <-
  phoneigen::LaplacianEigenmaps(adjacency_matrix)

eigenmaps

eigenmaps$Projection[[2]]
```

Recall that the optimal `n`-dimensional embedding of the data is given by the 
Laplacian eigenvectors associated with the `n` least non-zero eigenvalues. In
the output printed above, none of the `Eigenvalue`s are `0`; however, it would 
be incorrect to include the first Laplacian eigenvector `e0` in the 
low-dimensional embedding. By construction, the graph used in this example is 
connected; hence, theory indicates that the least eigenvalue should be `0`. 
While the least eigenvalue above is not identically `0`, its magnitude is less 
than a reasonable threshold of `sqrt(.Machine$double.eps)`. Consequently, the 
first Laplacian eigenvector is ignored, and only eigenvectors `e1` and above are 
considered. Here, we stress that `phoneigen::LaplacianEigenmaps` does not
suggest a threshold below which `Eigenvalue`s should be considered `0`.
Furthermore, if the constructed graph is not connected, more than one
eigenvalue will be `0`, by theory. Hence, users should be thoughtful when
interpreting the output of `phoneigen::LaplacianEigenmaps` and deciding which
Laplacian eigenvectors offer the optimal low-dimensional embedding.

A low-dimensional representation of the data can be easily pulled
from the computed `eigenmaps` with the function `phoneigen::ReduceDimensions`,
which takes as arguments a data frame, `x`; and a comma-separated list of
unquoted variables that exactly match names of `Eigenvector`s in `x`.

```{r, collapse = TRUE}
reduced <-
  phoneigen::ReduceDimensions(x = eigenmaps, e1, e2)

reduced
```

The low-dimensional data are then in a format that can be passed to a function
for statistical modeling (e.g., `lm` or `lme4::lmer`) or for plotting (e.g., 
`ggplot2::ggplot`). Figure 3 shows the distribution of the sibilant fricatives
in a two-dimensional space defined by the first and second non-zero Laplacian
eigenvectors. The first Laplacian eigenvector, `e1`, encodes the linguistic
differences, yielding linearly separable clusters for /s/ and /ʃ/. The second
Laplacian eigenvector, `e2`, encodes the indexical differences, indicating
talker sex. Along the second eigenvector, the men's productions are more
variable---an observation that could be more rigorously tested by collecting
gender typicallity ratings from adults listening to these productions and then
correlating the ratings against the values of the second eigenvector. While
the Laplacian eigenmaps algorithm exploited some metadata about the excitation
patterns (namely, the talker who produced the token and the word in which the
token was produced), the clusters that are plotted in Figure 3 indicate that
the algorithm generalized across the lexical identities of words to learn 
segmental categories, and across talker identities to learn categories for
talker sex.

```{r, echo = FALSE, results = "hide"}
fig3_caption <- stringr::str_c(
  "Figure 3: Distributions of first and second non-zero Laplacian eigenvectors",
  "learned from the sibilant fricatives' excitation patterns. Ellipses indicate",
  "95% confidence regions, assuming a bivariate t-distribution.",
  sep = " "
)
```

```{r, echo = FALSE, fig.cap = fig3_caption, fig.width = 7, fig.height = 5, fig.retina = 2, fig.align = "center"}
reduced %>%
  tidyr::separate(X, into = c("Participant", "Session", "Trial", "Orthography")) %>%
  dplyr::left_join(
    dplyr::select(phoneigen::SibilantFricatives(),
                  Participant, Session, Trial, Orthography, TargetC),
    by = c("Participant", "Session", "Trial", "Orthography")
  ) %>%
  dplyr::mutate(Sex = substring(Session, first = 7, last = 7)) %>%
  ggplot2::ggplot(mapping = ggplot2::aes(x = e1,  y = e2, color = Sex, 
                                         shape = TargetC, linetype = TargetC)) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom", legend.direction = "horizontal") +
  ggplot2::xlab("First non-zero Laplacian eigenvector") +
  ggplot2::ylab("Second non-zero Laplacian eigenvector") +
  ggplot2::scale_x_reverse(position = "top") +
  ggplot2::scale_shape_manual(name = "", 
                              values = c("s" = 1, "S" = 5), 
                              labels = c("/ʃ/", "/s/")) +
  ggplot2::scale_color_manual(name = "", 
                              values = c("F" = "#E98300", "M" = "#0039A6"),
                              labels = c("Female", "Male")) +
  ggplot2::scale_linetype_manual(name = "", 
                                 values = c("s" = "dashed", "S" = "dotted"),
                                 labels = c("/ʃ/", "/s/")) +
  ggplot2::geom_point(size = 3) +
  ggplot2::stat_ellipse(type = "t", level = 0.95)
```



### Linear discriminant analysis

In order to compare the results of Laplacian eigenmaps with those of another 
data-based dimensionality reduction algorithm, we apply linear discriminant
analysis (LDA) to the excitation patterns computed from sibilant fricatives.
The motivating idea behind this approach is to treat each component of an
excitation pattern as a variable that is observed for each data point (i.e.,
the first component of an excitation pattern is the amount of excitation in an
auditory filter centered at 3 ERB). While the reader may be more familiar with
LDA as a method for reducing the dimensionality of acoustic _features_ (e.g.,
time-varying sequences of spectral moments: Forrest et al., 1988), 
the phonetics literature is not without precedent where LDA has been applied to
spectral representations directly (e.g., Cassidy & Harrington, 1995).

A full description of the LDA algorithm falls outside of the scope of this 
tutorial; hence, we assume the reader has some familiarity with it. Briefly,
given `M` variables observed on data drawn from `N` classes, with `N << M`,
the LDA algorithm an `N-1`-dimensional representation of the data. Therefore,
an input to the LDA algorithm is the class membership of each data observation.
The `LDA` function, defined below, implements a procedure for applying LDA to
data. This function depends on the `caret::preProcess` function to first 
transform the data by normalizing mean, variance, and skewness of the variables;
and on the `MASS::lda` function to implement the core of the LDA algorithm.

```{r}
LDA <- function(classes, data) {
  data_dims <- purrr::map_dbl(data, length)
  if (dplyr::n_distinct(data_dims) != 1) {
    stop("Data observations do not have the same dimensionality")
  }
  data_dim <- unique(data_dims)
  n_classes <- dplyr::n_distinct(classes)
  lda_data_raw <- 
    data %>%
    purrr::simplify() %>%
    matrix(ncol = data_dim, byrow = TRUE) %>%
    tibble::as_data_frame() %>%
    purrr::set_names(stringr::str_c("D", 1:data_dim)) %>%
    as.data.frame()
  lda_preprocess <- caret::preProcess(x = lda_data_raw,
                                      method = c("BoxCox", "center", "scale"))
  lda_data <-
    tibble::tibble(Class = factor(classes)) %>%
    dplyr::bind_cols(predict(lda_preprocess, lda_data_raw)) %>%
    as.data.frame()
  suppressWarnings(
    lda_fit <- MASS::lda(formula = Class ~ ., data = lda_data,
                         prior = rep(1/n_classes, times = n_classes))
  )
  lda_embedding <-
    predict(lda_fit, lda_data)[["x"]] %>%
    tibble::as_data_frame() %>%
    dplyr::bind_cols(tibble::tibble(Class = classes))
  return(lda_embedding)
}
```

We fit two LDA models to the `phoneigen::SibilantFricatives()` data set. In the
first, the class of each data observation was determined by crossing the target
consonant with the sex of the talker. This class typology was motivated by the
categories that seem to have been learned by the first and second Laplacian
eigenvectors, respectively, in Figure 3. The results of this four-class model
are shown in the left panel of Figure 4. Four distinct clusters of data
are readily apparent. Compared with Figure 3, there seems to be less separation 
between the linguistic categories (/s/ vs. /ʃ/), but greater separation between
the indexical categories (female vs. male).

In the second model, the class of each data observation was determined by
crossing the particiapnt's identifier with the orthography of the target word,
which yielded 464 classes. This typology was motivated by the types of metadata
that were input to the Laplacian eigenmaps algorithm in order to construct the
adjacency matrix. The results of this second model are shown in the right panel
of Figure 4. Here, the first linear discriminant fairly well differentiates
the target consonant categories. However, the talker-sex categories are not
as well differentiated as in either the previous LDA model or the Laplacian
eigenmaps model.


```{r, echo = FALSE}
fig4_caption <- stringr::str_c(
  "Figure 4: Distributions of first and second linear discriminants learned from",
  "the sibilant fricatives' excitation patterns. The left panel shows linear",
  "discriminants learned with 4 input classes derived by crossing the target consonant",
  "and talker sex variables. The right panel shows linear discriminants learned with",
  " 464 input classes derived by crossing participant with target word.",
  "Ellipses indicate 95% confidence regions, assuming a bivariate t-distribution.",
  sep = " "
)
```

```{r, echo = FALSE, fig.cap = fig4_caption, fig.width = 7, fig.height = 5, fig.retina = 2, fig.align = "center"}
sibilants_lda_sex_consonant <-
  phoneigen::SibilantFricatives() %>%
  dplyr::mutate(Sex = substring(Session, first = 7, last = 7),
                Class = stringr::str_c(Sex, TargetC, sep = "_")) %>%
  dplyr::pull(Class) %>%
  LDA(data = dplyr::pull(phoneigen::SibilantFricatives(), ExcitationPattern)) %>%
  tidyr::separate(col = Class, into = c("Sex", "TargetC")) %>%
  ggplot2::ggplot(mapping = ggplot2::aes(x = LD1, y = LD2, color = Sex, 
                                         shape = TargetC, linetype = TargetC)) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom", legend.direction = "horizontal") +
  ggplot2::xlab("First linear discriminant") +
  ggplot2::ylab("Second linear discriminant") +
  ggplot2::scale_x_continuous(position = "top") +
  ggplot2::scale_shape_manual(name = "", 
                              values = c("s" = 1, "S" = 5), 
                              labels = c("/ʃ/", "/s/")) +
  ggplot2::scale_color_manual(name = "", 
                              values = c("F" = "#E98300", "M" = "#0039A6"),
                              labels = c("Female", "Male")) +
  ggplot2::scale_linetype_manual(name = "", 
                                 values = c("s" = "dashed", "S" = "dotted"),
                                 labels = c("/ʃ/", "/s/")) +
  ggplot2::geom_point(size = 3) +
  ggplot2::stat_ellipse(type = "t", level = 0.95)

sibilants_lda_participant_orthography <-
  phoneigen::SibilantFricatives() %>%
  dplyr::mutate(Class = stringr::str_c(Participant, Orthography, sep = "_")) %>%
  dplyr::pull(Class) %>%
  LDA(data = dplyr::pull(phoneigen::SibilantFricatives(), ExcitationPattern)) %>%
  tidyr::separate(col = Class, into = c("Participant", "Orthography")) %>%
  dplyr::left_join(
    dplyr::distinct(phoneigen::SibilantFricatives(),
                    Participant, Session, Orthography, TargetC),
    by = c("Participant", "Orthography")
  ) %>%
  dplyr::mutate(Sex = substring(Session, first = 7, last = 7)) %>%
  ggplot2::ggplot(mapping = ggplot2::aes(x = LD1, y = LD2, color = Sex, 
                                         shape = TargetC, linetype = TargetC)) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom", legend.direction = "horizontal") +
  ggplot2::xlab("First linear discriminant") +
  ggplot2::ylab("Second linear discriminant") +
  ggplot2::scale_x_reverse(position = "top") +
  ggplot2::scale_y_continuous(position = "right") +
  ggplot2::scale_shape_manual(name = "", 
                              values = c("s" = 1, "S" = 5), 
                              labels = c("/ʃ/", "/s/")) +
  ggplot2::scale_color_manual(name = "", 
                              values = c("F" = "#E98300", "M" = "#0039A6"),
                              labels = c("Female", "Male")) +
  ggplot2::scale_linetype_manual(name = "", 
                                 values = c("s" = "dashed", "S" = "dotted"),
                                 labels = c("/ʃ/", "/s/")) +
  ggplot2::geom_point(size = 3) +
  ggplot2::stat_ellipse(type = "t", level = 0.95)

grid::grid.newpage()
grid::pushViewport(grid::viewport(layout = grid::grid.layout(1, 2)))
print(sibilants_lda_sex_consonant, vp = grid::viewport(layout.pos.col = 1))
print(sibilants_lda_participant_orthography, vp = grid::viewport(layout.pos.col = 2))
```





## Stop bursts

The voiceless English stops /t, k/ are articulated by raising the tongue tip or
body to make contact with the roof of the mouth and fully occlude the oral
cavity. Sound is produced when the occlusion is released, causing a sudden
drop in intraoral pressure and generating a transient acoustic burst. The
place of articulation of the velar stop /k/ is subject to substantial vowel
contextual effects: when produced before a front vowel as compared with a 
back vowel, the occlusion is articulated at a much more anterior place.
Indeed, the /t/-vs.-/k/ contrast may be described as a tongue-place contrast, 
when these consonants occur before a back vowel, but as a tongue-posture contrast 
(apical vs. laminal), before front vowels. In this example, we apply Laplacian 
eigenmaps to spectral representations of stop bursts, and demonstrate that the 
learned low-dimensional embedding reflects the two articulatory axes of the 
/t/-vs.-/k/ contrast, namely, tongue place and tongue posture. 



### Laplacian eigenmaps

The steps for applying Laplacian eigenmaps to excitation patterns computed from
stop bursts follow much the same sequence as those for applying the algorithm
to sibilant fricatives above. As such, we don't repeat much of the exposition
that is offered above. Instead, we draw the readers attention to two points of 
our Laplacian eigenmaps analysis of the stop bursts' excitation patterns. The
first is syntactic in nature: the `phoneigen` functions may be chained together
using the forward-pipe operator `%>%` from the `magrittr` package. This construct
allows the user to pipe the component function calls together into a single
`R` expression as is demonstrated in the code listing below, which makes for,
in our opinion, more readable and secure code.

The second is a more substantive difference in how the graph's edges are 
weighted. Specifically, the graph construction below introduces an
_alignment parameter_ `mu`, such that `0 < mu < 1`, which tunes the degree to
which the Laplacian eigenmaps embedding emphasizes the edge weights assigned
within each talker, or the edge weights assigned to productions of the same
target word by different talkers. The alignment parameter value used below
`mu = 1/6` gives greater weight to the across-talker alignment than to the 
structure of the within-talker subgraphs. This value for `mu` was chosen based 
on pilot work with a subset of the data presented here (Reidy, Beckman, Edwards,
Munson & Johnson, 2017). 

```{r, eval = FALSE}
mu <- 1/6

phoneigen::StopBursts() %>%
  phoneigen::CartesianSquare(
    Participant, Session, Trial, Orthography, ExcitationPattern
  ) %>%
  phoneigen::WeightEdgesIf(
    condition = Participant_i == Participant_j,
    values = mu * exp(-Jeffrey(ExcitationPattern_i, ExcitationPattern_j))
  ) %>%
  phoneigen::WeightEdgesIf(
    condition = Participant_i != Participant_j & Orthography_i == Orthography_j,
    values = (1 - mu) * exp(-Jeffrey(ExcitationPattern_i, ExcitationPattern_j))
  ) %>%
  phoneigen::Normalize() %>%
  phoneigen::AdjacencyMatrix() %>%
  phoneigen::LaplacianEigenmaps() %>%
  phoneigen::ReduceDimensions(e1, e2)
```

The two-dimensional representation of the stop bursts, given by the first and
second non-zero Laplacian eigenvectors, is shown in Figure 5 below. Within this
space, three clusters seem to emerge. For target /t/, the productions from
front-vowel contexts melt into those from back-vowel contexts to form a single
cluster. This single /t/ cluster is differentiated from the productions of
target /k/ in back-vowel contexts, along the dimension of the first eigenvector;
and from the productions of target /k/ in front-vowel contexts, along the
second eigenvector. Furthermore, there is minimal overlap between any of the
three categories: only 5 productions of front-vowel /k/ are contained within
the convex hull of the target /t/ productions. 

The spectral representations input to the algorithm are "static"
in the sense that they were computed from a single window that spanned from
5 ms before the burst to 20 ms after the burst. A static spectral representation
of stop bursts conflicts with much work on the acoustics of stop bursts,
which have argued for and developed features that represent time-varying changes
to the spectral shape of the stop burst (e.g., Kewley-Port, 1983; Nossair &
Zahorian, 1991; Marin, Pouplier & Harrington, 2010). Despite the input 
representations not encoding any information about time-varying spectral change,
the Laplacian eigenmaps algorithm seems able to learn a low-dimensional 
representation for the /t/-vs.-/k/ contrast that reveals vowel-contextual 
allophonic variation and that reflects the underlying dimensionality of the 
articulatory system for producing these sounds.


```{r, echo = FALSE}
stop_bursts_reduced <-
  phoneigen::StopBursts() %>%
  phoneigen::CartesianSquare(
    Participant, Session, Trial, Orthography, ExcitationPattern
  ) %>%
  phoneigen::WeightEdgesIf(
    condition = Participant_i == Participant_j,
    values = (1/6) * exp(-Jeffrey(ExcitationPattern_i, ExcitationPattern_j))
  ) %>%
  phoneigen::WeightEdgesIf(
    condition = Participant_i != Participant_j & Orthography_i == Orthography_j,
    values = (5/6) * exp(-Jeffrey(ExcitationPattern_i, ExcitationPattern_j))
  ) %>%
  phoneigen::Normalize() %>%
  phoneigen::AdjacencyMatrix() %>%
  phoneigen::LaplacianEigenmaps() %>%
  phoneigen::ReduceDimensions(e1, e2)
```

```{r, echo = FALSE}
fig5_caption <- stringr::str_c(
  "Figure 5: Distributions of the first and second non-zero Laplacian eigenvectors",
  "learned from the stop bursts' excitation patterns. Ellipses indicate 95%",
  "confidence regions, assuming a bivariate t-dstribution.",
  sep = " "
)
```

```{r, echo = FALSE, fig.cap = fig5_caption, fig.width = 7, fig.height = 5, fig.retina = 2, fig.align = "center"}
stop_bursts_reduced %>%
  tidyr::separate(X, into = c("Participant", "Session", "Trial", "Orthography"), sep = "_") %>%
  dplyr::left_join(
    dplyr::select(phoneigen::StopBursts(),
                  Participant, Session, Trial, Orthography, TargetC, TargetV),
    by = c("Participant", "Session", "Trial", "Orthography")
  ) %>%
  dplyr::mutate(VowelPlace = ifelse(TargetV %in% c("i:", "I", "eI", "E", "@"),
                                    "Front", "Back")) %>%
  ggplot2::ggplot(mapping = ggplot2::aes(x = e1, y = e2, color = TargetC, 
                                         shape = VowelPlace, linetype = VowelPlace)) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom", legend.direction = "horizontal") +
  ggplot2::xlab("First non-zero Laplacian eigenvector") +
  ggplot2::ylab("Second non-zero Laplacian eigenvector") +
  ggplot2::scale_x_reverse(position = "top") +
  ggplot2::scale_shape_manual(name = "", values = c("Front" = 1, "Back" = 5),
                              labels = c("Back", "Front")) +
  ggplot2::scale_linetype_manual(name = "", values = c("Front" = "dashed", "Back" = "dotted"),
                                 labels = c("Back", "Front")) +
  ggplot2::scale_color_manual(name = "", values = c("th" = "#E98300", "kh" = "#008542"),
                              labels = c("/k/", "/t/")) +
  ggplot2::geom_point(size = 3) +
  ggplot2::stat_ellipse(type = "t", level = 0.95)
```



### Linear discriminant analysis

As with the sibilant fricatives, we compare the performance of Laplacian 
eigenmaps on the stop bursts to that of linear discriminant analysis (LDA). 
Again, there seem to be two reasonable choices for the class typology that is
input to the LDA algorithm: first, the 4-class typology got by crossing
target consonant and vowel context (front vs. back) variables; second, the
512-class typology got by crossing participant and target word variables.
Figure 6 shows the two-dimensional embedding determined by the first and second
linear discriminants learned from each of these two class typologies. Both
panels exhibit patterns that are qualitatively similar to what was observed 
above in Figure 5: productions of /t/ are differentiated from productions of
back-vowel-/k/ along the first linear discriminant; productions of /t/ are
differentiated from front-vowel-/k/ along the second linear discriminant.
Though, as would be expected, the separation along the second linear discriminant
is noticeably better in the left panel of Figure 6, where the class typology 
input to LDA matches the categories expected to be resolved by the algorithm.
Finally, we note that compared with the results of Laplacian eigenmaps, which 
takes as input no explicit information about the number of phonetic categories,
the 4-class LDA model yielded poorer separation between phonetic categories
observed in adult speech.

```{r, echo = FALSE}
fig6_caption <- stringr::str_c(
  "Figure 6: Distributions of first and second linear discriminants learned from",
  "the stop bursts' excitation patterns. The left panel shows linear discriminants",
  "learned with 4 input classes derived by crossing the target consonant and vowel",
  "context (front vowel vs. back vowel) variables. The right panel shows linear", 
  "discriminants learned with 512 input classes derived by crossing the participant and",
  "target word variables. Ellipses indicate 95% confidence regions, assuming a bivariate",
  "t-distribution.",
  sep = " "
)
```

```{r, echo = FALSE, fig.cap = fig6_caption, fig.width = 7, fig.height = 5, fig.retina = 2, fig.align = "center"}
stops_lda_consonant_vowel <-
  phoneigen::StopBursts() %>%
  dplyr::mutate(VowelPlace = ifelse(TargetV %in% c("i:", "I", "eI", "E", "@"),
                                    "Front", "Back"),
                Class = stringr::str_c(VowelPlace, TargetC, sep = "_")) %>% 
  dplyr::pull(Class) %>%
  LDA(data = dplyr::pull(phoneigen::StopBursts(), ExcitationPattern)) %>%
  tidyr::separate(col = Class, into = c("VowelPlace", "TargetC")) %>%
  ggplot2::ggplot(mapping = ggplot2::aes(x = LD1, y = LD2, color = TargetC, 
                                         shape = VowelPlace, linetype = VowelPlace)) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom", legend.direction = "horizontal") +
  ggplot2::xlab("First linear discriminant") +
  ggplot2::ylab("Second linear discriminant") +
  ggplot2::scale_x_continuous(position = "top") +
  ggplot2::scale_shape_manual(name = "", values = c("Front" = 1, "Back" = 5),
                              labels = c("Back", "Front")) +
  ggplot2::scale_linetype_manual(name = "", values = c("Front" = "dashed", "Back" = "dotted"),
                                 labels = c("Back", "Front")) +
  ggplot2::scale_color_manual(name = "", values = c("th" = "#E98300", "kh" = "#008542"),
                              labels = c("/k/", "/t/")) +
  ggplot2::geom_point(size = 3) +
  ggplot2::stat_ellipse(type = "t", level = 0.95)

stops_lda_participant_orthography <-
  phoneigen::StopBursts() %>%
  dplyr::mutate(Class = stringr::str_c(Participant, Orthography, sep = "_")) %>% 
  dplyr::pull(Class) %>%
  LDA(data = dplyr::pull(phoneigen::StopBursts(), ExcitationPattern)) %>%
  tidyr::separate(col = Class, into = c("Participant", "Orthography"), sep = "_") %>%
  dplyr::left_join(
    dplyr::distinct(phoneigen::StopBursts(),
                    Participant, Orthography, TargetC, TargetV),
    by = c("Participant", "Orthography")
  ) %>%
  dplyr::mutate(VowelPlace = ifelse(TargetV %in% c("i:", "I", "eI", "E", "@"), 
                                    "Front", "Back")) %>%
  ggplot2::ggplot(mapping = ggplot2::aes(x = LD1, y = LD2, color = TargetC, 
                                         shape = VowelPlace, linetype = VowelPlace)) +
  ggplot2::theme_bw() +
  ggplot2::theme(legend.position = "bottom", legend.direction = "horizontal") +
  ggplot2::xlab("First linear discriminant") +
  ggplot2::ylab("Second linear discriminant") +
  ggplot2::scale_x_reverse(position = "top") +
  ggplot2::scale_y_continuous(position = "right") +
  ggplot2::scale_shape_manual(name = "", values = c("Front" = 1, "Back" = 5),
                              labels = c("Back", "Front")) +
  ggplot2::scale_linetype_manual(name = "", values = c("Front" = "dashed", "Back" = "dotted"),
                                 labels = c("Back", "Front")) +
  ggplot2::scale_color_manual(name = "", values = c("th" = "#E98300", "kh" = "#008542"),
                              labels = c("/k/", "/t/")) +
  ggplot2::geom_point(size = 3) +
  ggplot2::stat_ellipse(type = "t", level = 0.95)

grid::grid.newpage()
grid::pushViewport(grid::viewport(layout = grid::grid.layout(1, 2)))
print(stops_lda_consonant_vowel, vp = grid::viewport(layout.pos.col = 1))
print(stops_lda_participant_orthography, vp = grid::viewport(layout.pos.col = 2))
```
